{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sentiment_analysis.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO4AOlyzD5CYwct1I2LD8Zj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DP8PpfJ-EQ3m","executionInfo":{"status":"ok","timestamp":1648287709716,"user_tz":-60,"elapsed":836,"user":{"displayName":"Victoria Beltrán","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXQMZ979rHuxCljx6cVTrirvnCC8_yIRyuVnt1XQ=s64","userId":"04685198138163414709"}},"outputId":"df7fc548-bc5c-448f-920a-e461141a76d4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['.config',\n"," 'TASS2017_T1_development.xml',\n"," 'TASS2017_T1_training.xml',\n"," 'ElhPolar_esV1.lex',\n"," 'TASS2017_T1_test.xml',\n"," 'sample_data']"]},"metadata":{},"execution_count":2}],"source":["import os\n","os.listdir('.')"]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PIWcAbRhMcIw","executionInfo":{"status":"ok","timestamp":1648287712305,"user_tz":-60,"elapsed":375,"user":{"displayName":"Victoria Beltrán","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXQMZ979rHuxCljx6cVTrirvnCC8_yIRyuVnt1XQ=s64","userId":"04685198138163414709"}},"outputId":"8eed3c73-1d42-4d75-cbe6-0457fa7ce663"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["# Exercice 2. Sentiment Analysis"],"metadata":{"id":"9EXESULRHRub"}},{"cell_type":"markdown","source":["## Modelo 1"],"metadata":{"id":"UaiAZlDnxjp7"}},{"cell_type":"code","source":["from sklearn import svm\n","from sklearn.metrics import classification_report\n","from nltk.tokenize import TweetTokenizer\n","from bs4 import BeautifulSoup\n","from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n","from sklearn.pipeline import Pipeline\n","import re\n","from sklearn import svm\n","from sklearn.metrics import classification_report\n","import scipy\n","import re\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline\n","\n","\n","train_path = \"TASS2017_T1_training.xml\"\n","dev_path = \"TASS2017_T1_development.xml\"\n","test_path = \"TASS2017_T1_test.xml\"\n","\n","def preprocess_xml(path, is_test=False):\n","    # Loading data from xml and tokenizing\n","    x=[]\n","    y=[]\n","    with open(path, \"r\") as f:\n","        soup = BeautifulSoup(f, \"xml\")\n","    tokenizer = TweetTokenizer(strip_handles=False, reduce_len=True, preserve_case=False)\n","\n","    for tweet in soup.find_all(\"tweet\"):\n","        content = tweet.content.string\n","        x.append(\" \".join(tokenizer.tokenize(content)))\n","        if not is_test:\n","          sentiment = tweet.sentiment.polarity.value.string\n","          y.append(sentiment)\n","\n","    return x,y\n","\n","\n","def mi_tokenizador(s):\n","    expresion = ['([0-9]+)(\\sde\\s)(\\w+)(\\sde\\s)([0-9]+)', '(http(s)*://)([^\\s]*)', '([0-9]{1,2})(\\/|-|:)([0-9]{1,2})(\\/|-|:)([0-9]{4}|[0-9]{2})',\n","                  '([^\\s]+@[^\\s]+)', '([@|#]([^\\s]+))', '([À-úA-Z]+\\.)+', '([0-9])+(\\.|,|\\-|\\:|\\/)([0-9]+)',  '([À-úa-zA-Z])+(\\-)([À-úa-zA-Z]+)',\n","                  '([^\\w\\s.:;,!¡/@¿?%~\\\"\\'\\#-])', '\\(', '\\)', '\\.\\.\\.', '\\.', '\\,', '\\'', '\\\"', '\\?', '\\¿', '\\!','\\¡', '\\;', '\\:','\\%','\\w+']\n","    reg_exp = \"|\".join(expresion)\n","    compilador= re.compile(reg_exp, re.U)\n","    return [i.group(0)  for i in re.finditer(compilador, s)]\n","\n","\n","def load_lexicon(path):\n","    polarities= {}\n","    with open(path, 'r') as f:\n","        line = f.readline()\n","        while line:\n","          if not line.startswith('#') and line!=\"\\n\":\n","              terms = line.split()\n","              term = terms[0]\n","              if terms[-1] == 'positive':\n","                  polarities[term] = \"POS\"\n","              elif terms[-1]=='negative':\n","                  polarities[term] = \"NEG\"\n","              else:\n","                  polarities[term] = \"NEU\"\n","          line = f.readline()\n","    return polarities\n","\n","\n","def get_polarities(lexicon, data):\n","  result=[]\n","  for line in data:\n","    positive = 0\n","    negative = 0\n","    for word in line.split():\n","      polarity = lexicon.get(word, \"NEU\")\n","      if polarity == \"POS\":\n","        positive +=1\n","      elif polarity == \"NEG\":\n","        negative +=1\n","    result.append([positive, negative])\n","  return result\n","\n","\n","# Data preprocessing\n","train_x, train_y = preprocess_xml(train_path)\n","dev_x, dev_y = preprocess_xml(dev_path)\n","test_x, test_y = preprocess_xml(test_path, is_test=True)\n","\n","# Vectorize the data\n","vectorizer = CountVectorizer(tokenizer=mi_tokenizador)\n","train_vectors = vectorizer.fit_transform(train_x)\n","dev_vectors = vectorizer.transform(dev_x)\n","\n","# External resources\n","lexicon = load_lexicon('ElhPolar_esV1.lex')\n","\n","# Get polarities of train data and append them to the train vectors\n","train_polaridades = get_polarities(lexicon, train_x)\n","#train_vectors=scipy.sparse.hstack([train_vectors, train_polaridades])\n","\n","# Train classifier\n","classifier_liblinear = svm.LinearSVC(C=0.1)\n","classifier_liblinear.fit(train_vectors, train_y)\n","\n","# Get polarities of the dev data and append them to the dev vectors\n","dev_polaridades = get_polarities(lexicon, dev_x)\n","#dev_vectors=scipy.sparse.hstack([dev_vectors, dev_polaridades])\n","\n","# Eval classifier with dev\n","prediction_liblinear = classifier_liblinear.predict(dev_vectors)\n","print(classification_report(dev_y, prediction_liblinear))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a_cqCVjqHXta","executionInfo":{"status":"ok","timestamp":1648287876771,"user_tz":-60,"elapsed":2545,"user":{"displayName":"Victoria Beltrán","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXQMZ979rHuxCljx6cVTrirvnCC8_yIRyuVnt1XQ=s64","userId":"04685198138163414709"}},"outputId":"d8292a3e-f455-4b5d-db24-09ddbcdedf83"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           N       0.60      0.79      0.68       219\n","         NEU       0.27      0.10      0.15        69\n","        NONE       0.26      0.11      0.16        62\n","           P       0.60      0.62      0.61       156\n","\n","    accuracy                           0.56       506\n","   macro avg       0.43      0.41      0.40       506\n","weighted avg       0.51      0.56      0.52       506\n","\n"]}]},{"cell_type":"markdown","source":["## Modelo 2"],"metadata":{"id":"CofiuzQrxnHQ"}},{"cell_type":"code","source":["from sklearn import svm\n","from sklearn.metrics import classification_report\n","from nltk.tokenize import TweetTokenizer\n","from bs4 import BeautifulSoup\n","from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n","from sklearn.pipeline import Pipeline\n","import re\n","from sklearn import svm\n","from sklearn.metrics import classification_report\n","import scipy\n","import re\n","import tensorflow as tf\n","import keras\n","import tensorflow_hub as hub\n","\n","\n","train_path = \"TASS2017_T1_training.xml\"\n","dev_path = \"TASS2017_T1_development.xml\"\n","test_path = \"TASS2017_T1_test.xml\"\n","dev_labels = []\n","\n","def preprocess_xml(path, is_test=False, is_dev=False):\n","    # Loading data from xml and tokenizing\n","    x=[]\n","    y=[]\n","    with open(path, \"r\") as f:\n","        soup = BeautifulSoup(f, \"xml\")\n","    tokenizer = TweetTokenizer(strip_handles=False, reduce_len=True, preserve_case=False)\n","\n","    for tweet in soup.find_all(\"tweet\"):\n","        content = tweet.content.string\n","        x.append(\" \".join(tokenizer.tokenize(content)))\n","        if not is_test:\n","          sentiment = tweet.sentiment.polarity.value.string\n","          if sentiment==\"N\":\n","            sentiment=0\n","          elif sentiment==\"P\":\n","            sentiment=1\n","          elif sentiment==\"NEU\":\n","            sentiment=2\n","          else:\n","            sentiment=3\n","          y.append(sentiment)\n","          if is_dev:\n","            dev_labels.append(sentiment)\n","    x = tf.data.Dataset.from_tensor_slices(x)\n","    y = keras.utils.np_utils.to_categorical(y, 4)\n","    y = tf.data.Dataset.from_tensor_slices(y)\n","    \n","    return x,y\n","\n","\n","# Data preprocessing\n","train_x, train_y = preprocess_xml(train_path)\n","train_dataset = tf.data.Dataset.zip((train_x, train_y))\n","\n","dev_x, dev_y = preprocess_xml(dev_path, is_dev=True)\n","dev_dataset = tf.data.Dataset.zip((dev_x, dev_y))\n","\n","test_x, test_y = preprocess_xml(test_path, is_test=True)\n","test_dataset = tf.data.Dataset.zip((test_x, test_y))\n","\n","train_dataset = train_dataset.batch(128)\n","dev_dataset = dev_dataset.batch(128)\n","test_dataset = test_dataset.batch(128)\n","\n","\n","\n","hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-es-dim128/1\", output_shape=[128],\n","                           input_shape=[], dtype=tf.string)\n","\n","model = keras.Sequential()\n","model.add(hub_layer)\n","model.add(keras.layers.Dense(16, activation='relu'))\n","model.add(keras.layers.Dense(4, activation='softmax'))\n","\n","model.summary()\n","\n","model.compile(optimizer='adam',\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","history = model.fit(train_dataset, epochs=20)\n","test_loss, test_acc = model.evaluate(dev_dataset)\n","print('\\nDev loss: {:.3f}, Dev accuracy: {:.3f}'.format(test_loss, test_acc))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EFwj-3utbLgT","executionInfo":{"status":"ok","timestamp":1648287874228,"user_tz":-60,"elapsed":4901,"user":{"displayName":"Victoria Beltrán","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXQMZ979rHuxCljx6cVTrirvnCC8_yIRyuVnt1XQ=s64","userId":"04685198138163414709"}},"outputId":"95445431-e650-4d98-fe7d-151e065e3dff"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," keras_layer_1 (KerasLayer)  (None, 128)               125009920 \n","                                                                 \n"," dense_2 (Dense)             (None, 16)                2064      \n","                                                                 \n"," dense_3 (Dense)             (None, 4)                 68        \n","                                                                 \n","=================================================================\n","Total params: 125,012,052\n","Trainable params: 2,132\n","Non-trainable params: 125,009,920\n","_________________________________________________________________\n","Epoch 1/20\n","8/8 [==============================] - 1s 4ms/step - loss: 1.3867 - accuracy: 0.2808\n","Epoch 2/20\n","8/8 [==============================] - 0s 5ms/step - loss: 1.3564 - accuracy: 0.4206\n","Epoch 3/20\n","8/8 [==============================] - 0s 4ms/step - loss: 1.3333 - accuracy: 0.4464\n","Epoch 4/20\n","8/8 [==============================] - 0s 4ms/step - loss: 1.3134 - accuracy: 0.4474\n","Epoch 5/20\n","8/8 [==============================] - 0s 4ms/step - loss: 1.2950 - accuracy: 0.4484\n","Epoch 6/20\n","8/8 [==============================] - 0s 4ms/step - loss: 1.2772 - accuracy: 0.4454\n","Epoch 7/20\n","8/8 [==============================] - 0s 4ms/step - loss: 1.2604 - accuracy: 0.4484\n","Epoch 8/20\n","8/8 [==============================] - 0s 5ms/step - loss: 1.2451 - accuracy: 0.4593\n","Epoch 9/20\n","8/8 [==============================] - 0s 4ms/step - loss: 1.2307 - accuracy: 0.4623\n","Epoch 10/20\n","8/8 [==============================] - 0s 5ms/step - loss: 1.2163 - accuracy: 0.4772\n","Epoch 11/20\n","8/8 [==============================] - 0s 4ms/step - loss: 1.2013 - accuracy: 0.4970\n","Epoch 12/20\n","8/8 [==============================] - 0s 4ms/step - loss: 1.1861 - accuracy: 0.5159\n","Epoch 13/20\n","8/8 [==============================] - 0s 4ms/step - loss: 1.1706 - accuracy: 0.5327\n","Epoch 14/20\n","8/8 [==============================] - 0s 4ms/step - loss: 1.1550 - accuracy: 0.5486\n","Epoch 15/20\n","8/8 [==============================] - 0s 4ms/step - loss: 1.1399 - accuracy: 0.5595\n","Epoch 16/20\n","8/8 [==============================] - 0s 5ms/step - loss: 1.1258 - accuracy: 0.5675\n","Epoch 17/20\n","8/8 [==============================] - 0s 4ms/step - loss: 1.1125 - accuracy: 0.5694\n","Epoch 18/20\n","8/8 [==============================] - 0s 4ms/step - loss: 1.1001 - accuracy: 0.5754\n","Epoch 19/20\n","8/8 [==============================] - 0s 4ms/step - loss: 1.0882 - accuracy: 0.5764\n","Epoch 20/20\n","8/8 [==============================] - 0s 4ms/step - loss: 1.0772 - accuracy: 0.5823\n","4/4 [==============================] - 0s 5ms/step - loss: 1.1088 - accuracy: 0.5711\n","\n","Dev loss: 1.109, Dev accuracy: 0.571\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.metrics import classification_report\n","\n","y_hat = model.predict(dev_dataset)\n","y_hat = [np.argmax(i) for i in y_hat]\n","print(len(y_hat))\n","print(len(dev_labels))\n","\n","print(classification_report(dev_labels, y_hat, target_names=[\"NEG\",\"POS\",\"NEU\",\"NONE\"]))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"htDkPPHVslz7","executionInfo":{"status":"ok","timestamp":1648287740575,"user_tz":-60,"elapsed":381,"user":{"displayName":"Victoria Beltrán","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXQMZ979rHuxCljx6cVTrirvnCC8_yIRyuVnt1XQ=s64","userId":"04685198138163414709"}},"outputId":"4e9772ea-806d-46f4-ad1e-89ff7303e4c9"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["506\n","506\n","              precision    recall  f1-score   support\n","\n","         NEG       0.54      0.83      0.66       219\n","         POS       0.54      0.58      0.56       156\n","         NEU       0.00      0.00      0.00        69\n","        NONE       0.67      0.03      0.06        62\n","\n","    accuracy                           0.54       506\n","   macro avg       0.44      0.36      0.32       506\n","weighted avg       0.48      0.54      0.46       506\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"markdown","source":["##Modelo 3"],"metadata":{"id":"23WHRTUHwhr5"}},{"cell_type":"code","source":["from sklearn import svm\n","from sklearn.metrics import classification_report\n","from nltk.tokenize import TweetTokenizer\n","from bs4 import BeautifulSoup\n","from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n","from sklearn.pipeline import Pipeline\n","import re\n","from sklearn import svm\n","from sklearn.metrics import classification_report\n","import scipy\n","import re\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline\n","\n","\n","train_path = \"TASS2017_T1_training.xml\"\n","dev_path = \"TASS2017_T1_development.xml\"\n","test_path = \"TASS2017_T1_test.xml\"\n","\n","def preprocess_xml(path, is_test=False):\n","    # Loading data from xml and tokenizing\n","    x=[]\n","    y=[]\n","    with open(path, \"r\") as f:\n","        soup = BeautifulSoup(f, \"xml\")\n","    tokenizer = TweetTokenizer(strip_handles=False, reduce_len=True, preserve_case=False)\n","\n","    for tweet in soup.find_all(\"tweet\"):\n","        content = tweet.content.string\n","        x.append(\" \".join(tokenizer.tokenize(content)))\n","        if not is_test:\n","          sentiment = tweet.sentiment.polarity.value.string\n","          y.append(sentiment)\n","\n","    return x,y\n","\n","\n","def mi_tokenizador(s):\n","    expresion = ['([0-9]+)(\\sde\\s)(\\w+)(\\sde\\s)([0-9]+)', '(http(s)*://)([^\\s]*)', '([0-9]{1,2})(\\/|-|:)([0-9]{1,2})(\\/|-|:)([0-9]{4}|[0-9]{2})',\n","                  '([^\\s]+@[^\\s]+)', '([@|#]([^\\s]+))', '([À-úA-Z]+\\.)+', '([0-9])+(\\.|,|\\-|\\:|\\/)([0-9]+)',  '([À-úa-zA-Z])+(\\-)([À-úa-zA-Z]+)',\n","                  '([^\\w\\s.:;,!¡/@¿?%~\\\"\\'\\#-])', '\\(', '\\)', '\\.\\.\\.', '\\.', '\\,', '\\'', '\\\"', '\\?', '\\¿', '\\!','\\¡', '\\;', '\\:','\\%','\\w+']\n","    reg_exp = \"|\".join(expresion)\n","    compilador= re.compile(reg_exp, re.U)\n","    return [i.group(0)  for i in re.finditer(compilador, s)]\n","\n","\n","def load_lexicon(path):\n","    polarities= {}\n","    with open(path, 'r') as f:\n","        line = f.readline()\n","        while line:\n","          if not line.startswith('#') and line!=\"\\n\":\n","              terms = line.split()\n","              term = terms[0]\n","              if terms[-1] == 'positive':\n","                  polarities[term] = \"POS\"\n","              elif terms[-1]=='negative':\n","                  polarities[term] = \"NEG\"\n","              else:\n","                  polarities[term] = \"NEU\"\n","          line = f.readline()\n","    return polarities\n","\n","\n","def get_polarities(lexicon, data):\n","  result=[]\n","  for line in data:\n","    positive = 0\n","    negative = 0\n","    for word in line.split():\n","      polarity = lexicon.get(word, \"NEU\")\n","      if polarity == \"POS\":\n","        positive +=1\n","      elif polarity == \"NEG\":\n","        negative +=1\n","    result.append([positive, negative])\n","  return result\n","\n","\n","# Data preprocessing\n","train_x, train_y = preprocess_xml(train_path)\n","dev_x, dev_y = preprocess_xml(dev_path)\n","test_x, test_y = preprocess_xml(test_path, is_test=True)\n","\n","# Vectorize the data\n","vectorizer = CountVectorizer(tokenizer=mi_tokenizador)\n","train_vectors2 = vectorizer.fit_transform(train_x)\n","dev_vectors2 = vectorizer.transform(dev_x)\n","\n","# External resources\n","lexicon = load_lexicon('ElhPolar_esV1.lex')\n","\n","# Get polarities of train data and append them to the train vectors\n","train_polaridades = get_polarities(lexicon, train_x)\n","train_vectors2=scipy.sparse.hstack([train_vectors2, train_polaridades])\n","\n","# Train classifier\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline\n","\n","classifier_liblinear2 = make_pipeline(StandardScaler(with_mean=False), SGDClassifier(max_iter=1000, tol=1e-3))\n","classifier_liblinear2.fit(train_vectors2, train_y)\n","\n","# Get polarities of the dev data and append them to the dev vectors\n","dev_polaridades = get_polarities(lexicon, dev_x)\n","dev_vectors2=scipy.sparse.hstack([dev_vectors2, dev_polaridades])\n","\n","# Eval classifier with dev\n","prediction_liblinear = classifier_liblinear2.predict(dev_vectors2)\n","print(classification_report(dev_y, prediction_liblinear))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1wUpGqv5wgkZ","executionInfo":{"status":"ok","timestamp":1648288015682,"user_tz":-60,"elapsed":2529,"user":{"displayName":"Victoria Beltrán","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXQMZ979rHuxCljx6cVTrirvnCC8_yIRyuVnt1XQ=s64","userId":"04685198138163414709"}},"outputId":"28fbef62-8341-439e-df68-0fd631eebd90"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           N       0.57      0.63      0.59       219\n","         NEU       0.21      0.17      0.19        69\n","        NONE       0.23      0.21      0.22        62\n","           P       0.51      0.50      0.51       156\n","\n","    accuracy                           0.47       506\n","   macro avg       0.38      0.38      0.38       506\n","weighted avg       0.46      0.47      0.47       506\n","\n"]}]},{"cell_type":"markdown","source":["# Modelo 4: combinación por votación del modelo 1, 2 y 3"],"metadata":{"id":"ZZYTscNpx6jx"}},{"cell_type":"code","source":["m1_ypred = classifier_liblinear.predict(dev_vectors)\n","m2_ypred = model.predict(dev_dataset)\n","target_names=[\"N\",\"P\",\"NEU\",\"NONE\"]\n","m2_ypred = [target_names[np.argmax(i)] for i in y_hat]\n","m3_ypred = classifier_liblinear2.predict(dev_vectors2)\n","\n","y_pred = []\n","for i in range(len(m1_ypred)):\n","  votes = np.array([m1_ypred[i], m2_ypred[i], m3_ypred[i]])\n","  values, counts = np.unique(votes, return_counts=True)\n","  idx = np.argmax(counts)\n","  y_pred.append(values[idx])\n","\n","\n","print(classification_report(dev_y, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F3rQlSiqwhEf","executionInfo":{"status":"ok","timestamp":1648288034586,"user_tz":-60,"elapsed":232,"user":{"displayName":"Victoria Beltrán","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXQMZ979rHuxCljx6cVTrirvnCC8_yIRyuVnt1XQ=s64","userId":"04685198138163414709"}},"outputId":"89e7af14-c965-4430-8748-9dac9892861b"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           N       0.51      0.90      0.65       219\n","         NEU       0.27      0.04      0.08        69\n","        NONE       0.46      0.10      0.16        62\n","           P       0.65      0.39      0.49       156\n","\n","    accuracy                           0.53       506\n","   macro avg       0.47      0.36      0.34       506\n","weighted avg       0.51      0.53      0.46       506\n","\n"]}]}]}