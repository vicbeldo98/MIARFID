{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DP8PpfJ-EQ3m",
        "outputId": "b935a679-8553-4067-99ad-0d46a406512c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'TASS2017_T1_development.xml',\n",
              " 'TASS2017_T1_training.xml',\n",
              " 'test_labels.txt',\n",
              " 'ElhPolar_esV1.lex',\n",
              " 'TASS2017_T1_test.xml',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "import os\n",
        "os.listdir('.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIWcAbRhMcIw",
        "outputId": "c5a0ab2a-3a1a-412b-d242-a07f2b510975"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercice 2. Sentiment Analysis"
      ],
      "metadata": {
        "id": "9EXESULRHRub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo 1"
      ],
      "metadata": {
        "id": "UaiAZlDnxjp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import re\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report\n",
        "import scipy\n",
        "import re\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "\n",
        "train_path = \"TASS2017_T1_training.xml\"\n",
        "dev_path = \"TASS2017_T1_development.xml\"\n",
        "test_path = \"TASS2017_T1_test.xml\"\n",
        "\n",
        "def preprocess_xml(path, is_test=False):\n",
        "    # Loading data from xml and tokenizing\n",
        "    x=[]\n",
        "    y=[]\n",
        "    ids=[]\n",
        "    with open(path, \"r\") as f:\n",
        "        soup = BeautifulSoup(f, \"xml\")\n",
        "    tokenizer = TweetTokenizer(strip_handles=False, reduce_len=True, preserve_case=False)\n",
        "\n",
        "    for tweet in soup.find_all(\"tweet\"):\n",
        "        content = tweet.content.string\n",
        "        x.append(\" \".join(tokenizer.tokenize(content)))\n",
        "        ids.append(tweet.tweetid.string)\n",
        "\n",
        "        if not is_test:\n",
        "          sentiment = tweet.sentiment.polarity.value.string\n",
        "          y.append(sentiment)\n",
        "\n",
        "    return x,y, ids\n",
        "\n",
        "\n",
        "def mi_tokenizador(s):\n",
        "    expresion = ['([0-9]+)(\\sde\\s)(\\w+)(\\sde\\s)([0-9]+)', '(http(s)*://)([^\\s]*)', '([0-9]{1,2})(\\/|-|:)([0-9]{1,2})(\\/|-|:)([0-9]{4}|[0-9]{2})',\n",
        "                  '([^\\s]+@[^\\s]+)', '([@|#]([^\\s]+))', '([À-úA-Z]+\\.)+', '([0-9])+(\\.|,|\\-|\\:|\\/)([0-9]+)',  '([À-úa-zA-Z])+(\\-)([À-úa-zA-Z]+)',\n",
        "                  '([^\\w\\s.:;,!¡/@¿?%~\\\"\\'\\#-])', '\\(', '\\)', '\\.\\.\\.', '\\.', '\\,', '\\'', '\\\"', '\\?', '\\¿', '\\!','\\¡', '\\;', '\\:','\\%','\\w+']\n",
        "    reg_exp = \"|\".join(expresion)\n",
        "    compilador= re.compile(reg_exp, re.U)\n",
        "    return [i.group(0)  for i in re.finditer(compilador, s)]\n",
        "\n",
        "\n",
        "# Data preprocessing\n",
        "train_x, train_y, _ = preprocess_xml(train_path)\n",
        "dev_x, dev_y, _ = preprocess_xml(dev_path)\n",
        "test_x, test_y, test_ids = preprocess_xml(test_path, is_test=True)\n",
        "\n",
        "# Vectorize the data\n",
        "vectorizer = CountVectorizer(tokenizer=mi_tokenizador)\n",
        "train_vectors = vectorizer.fit_transform(train_x)\n",
        "dev_vectors = vectorizer.transform(dev_x)\n",
        "\n",
        "# Train classifier\n",
        "classifier_liblinear = svm.LinearSVC(C=0.1)\n",
        "classifier_liblinear.fit(train_vectors, train_y)\n",
        "\n",
        "# Eval classifier with dev\n",
        "prediction_liblinear = classifier_liblinear.predict(dev_vectors)\n",
        "print(classification_report(dev_y, prediction_liblinear))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_cqCVjqHXta",
        "outputId": "eaadd477-29c5-4d3e-f8a2-969b9475ddf8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           N       0.60      0.79      0.68       219\n",
            "         NEU       0.27      0.10      0.15        69\n",
            "        NONE       0.26      0.11      0.16        62\n",
            "           P       0.60      0.62      0.61       156\n",
            "\n",
            "    accuracy                           0.56       506\n",
            "   macro avg       0.43      0.41      0.40       506\n",
            "weighted avg       0.51      0.56      0.52       506\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_vectors = vectorizer.transform(test_x)\n",
        "predicted_labels = classifier_liblinear.predict(test_vectors)\n",
        "with open(\"test_labels.txt\", 'w') as f:\n",
        "  for i in range(len(test_ids)):\n",
        "    id = test_ids[i]\n",
        "    text = test_x[i]\n",
        "    pred = predicted_labels[i]\n",
        "    f.write(id+'\\t'+pred+'\\n')"
      ],
      "metadata": {
        "id": "dOUZ6IK0uNue"
      },
      "execution_count": 45,
      "outputs": []
    }
  ]
}