{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tokenizar_expresiones_reg.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Autora: Victoria BeltrÃ¡n DomÃ­nguez\n",
        "## Practica 1: Tokenizador para el espaÃ±ol\n",
        "\n",
        "## Objetivo: \n",
        "\n",
        "ManipulaciÃ³n de cadenas y uso de expresiones regulares para construir un tokenizador para el espaÃ±ol con una serie de restricciones.\n",
        "\n",
        "\n",
        "## Referencias:\n",
        "- [Tutorial](http://docs.python.org/tutorial/index.html)\n",
        "- [String Processing](http://docs.python.org/library/string.html)\n",
        "- [Strin Processing 2](http://docs.python.org/tutorial/introduction.html#strings)\n",
        "- [Regular Expressions](http://docs.python.org/library/re.html)\n",
        "- [Regular Expressions 2](http://docs.python.org/howto/regex.html#regex-howto)\n",
        "\n",
        "Construir un tokenizador para el espaÃ±ol, que, dado un fichero de texto de entrada (entrada_tokenizador.txt), separe en tokens, y los muestre en un fichero de salida en el formato que se muestra en (salida_tokenizador.txt). Por lo menos el tokenizador deberÃ¡ funcionar correctamente para el ejemplo.\n",
        "\n",
        "El tokenizador debe cumplir las siguientes restricciones:\n",
        "\n",
        "1) Los sÃ­mbolos que hay que separar de cada palabra son: ( ) . , â€˜ â€œ ? Â¿ ! Â¡ â€¦ ; :\n",
        "\n",
        "2) No se deben separar los nÃºmeros decimales, ejemplo: 44,45 45.60\n",
        "\n",
        "3) No se deben separar fechas 12/12/90, 12-03-98, ni horas, 9:30 h.\n",
        "\n",
        "4) Las fechas en formato 12 de febrero de 2018 hay que mantenerlas como un token\n",
        "\n",
        "5) No se deben separar direcciones web http://www.colorin.com ni correos electrÃ³nicos\n",
        "xx@cdit.com\n",
        "\n",
        "6) Hay que mantener menciones a usuarios (@user) y hashtags (#hashtag) como se\n",
        "utilizan en Twitter.\n",
        "\n",
        "7) Hay que mantener acrÃ³nimos, por ejemplo: EE.UU., S.L., CC.OO., S.A., D., U.R.S.S, â€¦\n",
        "\n",
        "8) Respetar los emoticonos: ï¿½ï¿½ï¿½ï¿½ etc."
      ],
      "metadata": {
        "id": "Ssuwgu6TKocY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d7FEwtGHUox",
        "outputId": "82b5e8d1-5678-424c-824a-7252945f9cfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ã‰l, Antonio, no vendrÃ¡ maÃ±ana: lo harÃ¡ pasado maÃ±ana.  Â¿Â¿Â¿Â¿CuÃ¡ndo???? No te lo he dicho... Â¡?Vale! no te he oido.\n",
            "De acuerdo; No irÃ©. Pesa 44.44 kg y mide 32,32 m. El 12-12-2020, y el 13/12: habrÃ¡ examen, el 14-12 ya veremos.\n",
            "El pseudo-cÃ³digo vale 30,6 sobre 100. El 15.5% no es suficiente. El \"bote\" estÃ¡ lleno, o 'vacio' no semi-vacio.\n",
            "Â¡Ay! el correo es fpla@dsic.upv.es y la web: http://users.dsic.upv.es/~fpla/ se me olvidaba, ha cambiado. Ahora es http://personales.upv.es/~fpla/ \n",
            "MaÃ±ana nos vemos a las 9:30 horas. 3/4 partes de la poblaciÃ³n come carne.\n",
            "el usuario @antonio_123 escribiÃ³ un tweet con el hashtag #alc-2019 el viernes, https://haha.ls-ps.com\n",
            "El 14 de marzo de 2021 empiezan las clases de LNRI de prÃ¡cticas y alguna cosa mÃ¡s.\n",
            "Todo lo que sigue son ejemplos de acrÃ³nimos que no se deberÃ­an separar: EE.UU., S.L., CC.OO., S.A., D., U.R.S.S., entre otros.\n",
            "PodÃ©is probar con otros ejemplos, e incluso, plantear algÃºn tipo de tokens que os interese: disfrutaaaddd!!!!! ðŸ™‚ \n",
            "'ðŸ¤” ðŸ™ˆ es asÃ­,bla, bla, bla  ðŸŽ“ es, se . ðŸ˜Œ de...; ðŸ’•ðŸ‘­ðŸ‘™ðŸ˜Š'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with open('entrada_tokenizador.txt', 'r') as f:\n",
        "    raw_data = f.read()\n",
        "print(raw_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(data, output_filename='salida_propia.txt'):\n",
        "    import re\n",
        "    expresion = ['([0-9]+)(\\sde\\s)(\\w+)(\\sde\\s)([0-9]+)', '(http(s)*://)([^\\s]*)', '([0-9]{1,2})(\\/|-|:)([0-9]{1,2})(\\/|-|:)([0-9]{4}|[0-9]{2})',\n",
        "                 '([^\\s]+@[^\\s]+)', '([@|#]([^\\s]+))', '([Ã€-ÃºA-Z]+\\.)+', '([0-9])+(\\.|,|\\-|\\:|\\/)([0-9]+)',  '([Ã€-Ãºa-zA-Z])+(\\-)([Ã€-Ãºa-zA-Z]+)',\n",
        "                 '([^\\w\\s.:;,!Â¡/@Â¿?%~\\\"\\'\\#-])', '\\(', '\\)', '\\.\\.\\.', '\\.', '\\,', '\\'', '\\\"', '\\?', '\\Â¿', '\\!','\\Â¡', '\\;', '\\:','\\%','\\w+']\n",
        "    reg_exp = \"|\".join(expresion)\n",
        "    compilador= re.compile(reg_exp, re.U)\n",
        "\n",
        "    with open(output_filename, 'w+') as fw:\n",
        "      for line in data.split(\"\\n\"):\n",
        "        if len(line) > 0:\n",
        "          fw.write(line + '\\n')\n",
        "          fw.write('\\n')\n",
        "          for i in re.finditer(compilador, line):\n",
        "            fw.write(line[i.start():i.end()] + '\\n')\n",
        "      fw.write('\\n')\n",
        "\n",
        "tokenize(raw_data)"
      ],
      "metadata": {
        "id": "qAmQ3U1bMOLk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! diff salida_tokenizador.txt salida_propia.txt"
      ],
      "metadata": {
        "id": "As7GbyA7spAM"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}