{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.listdir('/kaggle/input/')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-25T10:43:44.352275Z","iopub.execute_input":"2022-05-25T10:43:44.352944Z","iopub.status.idle":"2022-05-25T10:43:44.389947Z","shell.execute_reply.started":"2022-05-25T10:43:44.352831Z","shell.execute_reply":"2022-05-25T10:43:44.388818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport json\nimport string\nfrom tqdm import tqdm\nimport pickle\nfrom time import time\nimport numpy as np\nfrom PIL import Image\nimport pandas as pd\n\nimport keras.preprocessing.image\nfrom keras.models import Sequential\nfrom keras.layers import (\n    LSTM, Embedding, TimeDistributed, Dense, RepeatVector, \n    Activation, Flatten, Reshape, concatenate, Dropout, \n    BatchNormalization, GlobalAveragePooling2D, Conv2D,\n    Activation, Add\n)\nfrom tensorflow.keras.optimizers import Adam , RMSprop\nfrom tensorflow.keras import Input, layers\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import add\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nimport tensorflow.keras.applications.inception_v3\nfrom tensorflow.keras.applications import DenseNet169\nimport tensorflow.keras.applications\nimport tensorflow as tf\n\nfrom tensorflow.keras.layers import TextVectorization\nfrom tensorflow.keras.applications import efficientnet\n\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nSTART = \"aaprincipioaa\"\nSTOP = \"zzfinzz\"\n\nIMAGE_SIZE = (299, 299)\nEMBEDDING_DIM = 512\nNUM_HEADS = 2\nFF_DIM = 32\nEPOCHS = 40\nBATCH_SIZE = 128\nLSTM_BATCH_SIZE = 8\nAUTOTUNE = tf.data.AUTOTUNE\nWORD_COUNT_THRESHOLD = 3\n\n\n# Root captioning contiene flickr-image-dataset y glove6B\nROOT_CAPTIONING = os.path.join(\"/\", \"kaggle\", \"input\")\nDATASET_DIR = os.path.join(ROOT_CAPTIONING, \"flickr-image-dataset\", \"flickr30k_images\")\nMODEL_DIR = os.path.join(\"/\", \"kaggle\", \"working\")","metadata":{"execution":{"iopub.status.busy":"2022-05-25T10:43:47.045274Z","iopub.execute_input":"2022-05-25T10:43:47.045642Z","iopub.status.idle":"2022-05-25T10:43:53.754649Z","shell.execute_reply.started":"2022-05-25T10:43:47.045581Z","shell.execute_reply":"2022-05-25T10:43:53.753678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sequence preprocess","metadata":{}},{"cell_type":"code","source":"null_punct = str.maketrans('', '', string.punctuation)\n\nlookup = {}\n\ntrain_descriptions = dict()\nvalid_descriptions = dict()\ntest_descriptions = dict()\nall_text_captions = list()\n\nmax_length = 0\nmax_length_caption = 0\nmax_length_image_path = 0\nword_counts = {}\n\nwith open( os.path.join(ROOT_CAPTIONING, \"flickr30k-split-json\", \"dataset_flickr30k.json\") ) as f:\n    reference_json = json.load(f)\n    \nfor image_dict in reference_json['images']:\n    \n    if image_dict['split'] == \"train\":\n        dict_to_add = train_descriptions\n    elif image_dict['split'] == \"val\":\n        dict_to_add = valid_descriptions\n    elif image_dict['split'] == \"test\":\n        dict_to_add = test_descriptions\n    \n    image_id = os.path.join(DATASET_DIR, 'flickr30k_images', image_dict['filename'])\n    image_captions = image_dict['sentences']\n    \n    if len(image_captions) != 5:\n        continue\n    \n    dict_to_add[image_id] = list()\n    \n    for caption_dict in image_captions:\n        \n        caption_str = caption_dict['raw']\n        \n        tokens = caption_str.split()\n        \n        lower_tokens = [word.lower() for word in tokens]\n        clean_tokens = [w.translate(null_punct) for w in lower_tokens]\n        description_tokens = [word for word in clean_tokens if word.isalpha()]\n\n        if len(description_tokens) > max_length:\n            max_length = len(description_tokens)\n            max_length_caption = caption_str\n            max_length_image_path = image_id\n\n        for word in description_tokens:\n            word_counts[word] = word_counts.get(word, 0) + 1\n\n        caption_str = ' '.join(description_tokens)\n        caption_str = START + \" \" + caption_str + \" \" + STOP\n        dict_to_add[image_id].append(caption_str)\n        all_text_captions.append(caption_str)\n\n        \nfor k, v in train_descriptions.items():\n    for desc in v:\n        print(desc)\n    break\n    \n# Hemos añadido dos tokens\nmax_length = max_length + 2\n\nn_total = len(reference_json['images'])\nn_train = len(train_descriptions)\nn_valid = len(valid_descriptions)\nn_test = len(test_descriptions)\n\nprint(f\"Número de imágenes: {len(reference_json['images'])}\\n\")\nprint(f\"Tamaño del vocabulario: {len(word_counts)}\\n\")\nvocab = [w for w in word_counts if word_counts[w] >= WORD_COUNT_THRESHOLD]\nvocab_size = len(vocab) + 2\nprint(f\"Tamaño del vocabulario si consideramos sólo palabras que aparecen almenos {WORD_COUNT_THRESHOLD} veces: {vocab_size}\\n\")\nprint(f\"Tamaño de la secuencia más larga: {max_length}\\n\")\nprint(f\"Secuencia más larga: {max_length_caption}\\n\")\nimg = mpimg.imread(max_length_image_path)\nimgplot = plt.imshow(img)\nprint(\"Imagen:\")\nplt.show()\n\nMAX_LENGTH_SEQ = max_length\n\nvectorization = TextVectorization(\n    max_tokens=vocab_size,\n    output_mode=\"int\",\n    output_sequence_length=MAX_LENGTH_SEQ\n)\n\nprint( f\"Número de descripciones: {len(all_text_captions)}\\n\")\nvectorization.adapt(all_text_captions)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T10:44:00.503018Z","iopub.execute_input":"2022-05-25T10:44:00.503345Z","iopub.status.idle":"2022-05-25T10:44:20.519726Z","shell.execute_reply.started":"2022-05-25T10:44:00.503314Z","shell.execute_reply":"2022-05-25T10:44:20.518622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM","metadata":{}},{"cell_type":"markdown","source":"### 1.  Create dataset","metadata":{}},{"cell_type":"code","source":"vocab = vectorization.get_vocabulary()\nvocab_lookup = dict(zip(vocab, range(len(vocab))))\nmax_decoded_sentence_length = MAX_LENGTH_SEQ - 1\nvalid_images = list(valid_descriptions.keys())\nEND_TOKEN_ID = vocab_lookup[STOP]\n\nwith open(os.path.join(ROOT_CAPTIONING,'1664-img-emb-densenet169', 'images','images.pkl'), \"rb\") as fp:\n    all_encoding = pickle.load(fp)\n\nprint(len(all_encoding))\n\ndef decode_and_resize(img_path):\n    img = tf.io.read_file(img_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, IMAGE_SIZE)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    return img\n\ndef data_generator(descriptions, num_photos_per_batch, img_embeds):\n    n = 0\n    x1, x2, y = [], [], []\n    while True:\n        for key, desc_list in descriptions.items():\n            #print(key)\n            n+=1\n            photo = img_embeds[key.split(\"/\")[-1]] #decode_and_resize(key)\n            # Each photo has 5 descriptions\n            for desc in desc_list:\n                # Convert each word into a list of sequences.\n                seq = vectorization(desc)\n                # Generate a training case for every possible sequence and outcome\n                index = 1\n                while seq[index] != END_TOKEN_ID:\n                    in_seq, out = seq[:index], seq[index]\n                    in_seq = pad_sequences([in_seq], maxlen=MAX_LENGTH_SEQ, padding='post')[0]\n                    out = to_categorical([out], num_classes=vocab_size)[0]\n                    x1.append(photo)\n                    x2.append(in_seq)\n                    y.append(out)\n                    index += 1\n                    \n            if n==num_photos_per_batch:\n                yield ([np.array(x1), np.array(x2)], np.array(y))\n                x1, x2, y = [], [], []\n                n=0\n\ntrain_steps = len(train_descriptions)//LSTM_BATCH_SIZE\nvalid_steps = len(valid_descriptions)//LSTM_BATCH_SIZE\ntrain_generator = data_generator(train_descriptions, LSTM_BATCH_SIZE, all_encoding)\nvalid_generator = data_generator(valid_descriptions, LSTM_BATCH_SIZE, all_encoding)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T10:44:30.846962Z","iopub.execute_input":"2022-05-25T10:44:30.847452Z","iopub.status.idle":"2022-05-25T10:44:34.338787Z","shell.execute_reply.started":"2022-05-25T10:44:30.847412Z","shell.execute_reply":"2022-05-25T10:44:34.337767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Error debugging","metadata":{}},{"cell_type":"code","source":"cont = 0\nproblematic = dict()\nfor key, desc_list in train_descriptions.items():\n    cont += 1\n    if cont > 2912:\n        problematic[key] = desc_list\n        print(key)\n    if cont > 2920:\n        break\n        \nproblematic_generator = data_generator(problematic, 1, all_encoding)\nfor i in range(len(problematic)):\n    b = next(problematic_generator)\n    print(b)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T10:20:38.079288Z","iopub.execute_input":"2022-05-25T10:20:38.079679Z","iopub.status.idle":"2022-05-25T10:20:38.085888Z","shell.execute_reply.started":"2022-05-25T10:20:38.079646Z","shell.execute_reply":"2022-05-25T10:20:38.084925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_size = 1664\nphrase_size = 80\nword_size = 9936\nfor i in range(train_steps):\n    if i % 20 == 0:\n        print(i)\n    b = next(train_generator)\n    b_0_0, b_0_1, b_1 = b[0][0].shape, b[0][1].shape, b[1].shape\n    print(b_0_0, b_0_1, b_1)\n    if b_0_0[1] != img_size or b_0_1[1] != phrase_size or b_1[1] != word_size:\n        print(\"ERROR\")\n        break\n\nprint(b_0_0, b_0_1, b_1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.  Create arquitecture","metadata":{}},{"cell_type":"code","source":"glove_dir = os.path.join(ROOT_CAPTIONING,'glove6b')\nembeddings_index = {} \nf = open(os.path.join(glove_dir, 'glove.6B.200d.txt'), encoding=\"utf-8\")\n\nfor line in tqdm(f):\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\n\nf.close()\nprint(f'Found {len(embeddings_index)} word vectors.')","metadata":{"execution":{"iopub.status.busy":"2022-05-25T10:44:55.208609Z","iopub.execute_input":"2022-05-25T10:44:55.208956Z","iopub.status.idle":"2022-05-25T10:45:22.536776Z","shell.execute_reply.started":"2022-05-25T10:44:55.208926Z","shell.execute_reply":"2022-05-25T10:45:22.534993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = vectorization.get_vocabulary()\nindex_lookup = dict(zip(range(len(vocab)), vocab))\n\nembedding_dim = 200\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\nfor i, word in index_lookup.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\nprint(embedding_matrix.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T10:45:30.94249Z","iopub.execute_input":"2022-05-25T10:45:30.942978Z","iopub.status.idle":"2022-05-25T10:45:31.0131Z","shell.execute_reply.started":"2022-05-25T10:45:30.942943Z","shell.execute_reply":"2022-05-25T10:45:31.01185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ninputs1 = Input(shape=(299,299,3,))\nencode_model = DenseNet169(weights='imagenet', include_top=False, input_tensor=inputs1)\nencode_model.trainable = False\nimg_emb = GlobalAveragePooling2D()(encode_model.output)\n\nfe1 = Dropout(0.5)(img_emb)\nfe2 = Dense(256, activation='relu')(fe1)\n\"\"\"\n\ninputs1 = Input(shape=(1664,))\nfe1 = Dropout(0.5)(inputs1)\nfe2 = Dense(256, activation='relu')(fe1)\n\n\ninputs2 = Input(shape=(MAX_LENGTH_SEQ,))\nse1 = Embedding(vocab_size, embedding_dim, mask_zero=True, name=\"pretrained_glove\")(inputs2)\nse2 = Dropout(0.5)(se1)\nse3 = LSTM(256)(se2)\n\ndecoder1 = add([fe2, se3])\ndecoder2 = Dense(256, activation='relu')(decoder1)\noutputs = Dense(vocab_size, activation='softmax')(decoder2)\ncaption_model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n#caption_model.summary()\n\ncaption_model.get_layer(\"pretrained_glove\").set_weights([embedding_matrix])\ncaption_model.get_layer(\"pretrained_glove\").trainable = False\ncaption_model.compile(loss='categorical_crossentropy', optimizer='adam')","metadata":{"execution":{"iopub.status.busy":"2022-05-25T10:45:34.099729Z","iopub.execute_input":"2022-05-25T10:45:34.100037Z","iopub.status.idle":"2022-05-25T10:45:35.164575Z","shell.execute_reply.started":"2022-05-25T10:45:34.100005Z","shell.execute_reply":"2022-05-25T10:45:35.163571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Train","metadata":{}},{"cell_type":"code","source":"model_path = os.path.join(\".\",f'caption-model-lstm-7-epochs.hdf5')\ncaption_model.load_weights(\"./caption-model-lstm.hdf5\")\n       \ncaption_model.fit(\n    train_generator,\n    epochs=6,\n    steps_per_epoch=train_steps,\n    verbose=1,\n    validation_data=valid_generator,\n    validation_steps=valid_steps\n)\n\ncaption_model.save_weights(model_path)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T12:22:46.762609Z","iopub.execute_input":"2022-05-25T12:22:46.762946Z","iopub.status.idle":"2022-05-25T21:20:33.345508Z","shell.execute_reply.started":"2022-05-25T12:22:46.762915Z","shell.execute_reply":"2022-05-25T21:20:33.344394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Evaluate","metadata":{}},{"cell_type":"markdown","source":"### 5. Generate captions","metadata":{}},{"cell_type":"code","source":"vocab = vectorization.get_vocabulary()\nindex_lookup = dict(zip(range(len(vocab)), vocab))\nmax_decoded_sentence_length = MAX_LENGTH_SEQ - 1\nvalid_images = list(valid_descriptions.keys())\n\ndef generate_caption():\n    # Select a random image from the validation dataset\n    sample_img = np.random.choice(valid_images)\n    print(sample_img)\n    \n    # Read the image from the disk\n    img = decode_and_resize(sample_img)\n    img = img.numpy().clip(0, 255).astype(np.uint8)\n    plt.imshow(img)\n    plt.show()\n\n    img_embed = all_encoding[sample_img.split(\"/\")[-1]]\n    img_embed = np.expand_dims(img_embed, axis=0)\n\n    decoded_caption = START\n    for i in range(max_decoded_sentence_length):\n        tokenized_caption = vectorization([decoded_caption])\n        predictions = caption_model.predict([img_embed, tokenized_caption])\n        sampled_token_index = np.argmax(predictions[0, :])\n        sampled_token = index_lookup[sampled_token_index]\n        decoded_caption += \" \" + sampled_token\n        if sampled_token == STOP:\n            break\n        \n\n    decoded_caption = decoded_caption.replace(START, \"\")\n    decoded_caption = decoded_caption.replace(STOP, \"\").strip()\n    print(\"Predicted Caption: \", decoded_caption)\n\n# Check predictions for a few samples\ngenerate_caption()\ngenerate_caption()\ngenerate_caption()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:20:52.700353Z","iopub.execute_input":"2022-05-25T21:20:52.700662Z","iopub.status.idle":"2022-05-25T21:21:08.334022Z","shell.execute_reply.started":"2022-05-25T21:20:52.700623Z","shell.execute_reply":"2022-05-25T21:21:08.331974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TRANSFORMER","metadata":{}},{"cell_type":"markdown","source":"### 1.  Create tensorflow dataset","metadata":{}},{"cell_type":"code","source":"def decode_and_resize(img_path):\n    img = tf.io.read_file(img_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, IMAGE_SIZE)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    return img\n\n\ndef process_input(img_path, captions):\n    return decode_and_resize(img_path), vectorization(captions)\n\n\ndef make_dataset(images, captions):\n    dataset = tf.data.Dataset.from_tensor_slices((images, captions))\n    dataset = dataset.shuffle(len(images))\n    dataset = dataset.map(process_input, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n\n    return dataset\n\n\n# Pass the list of images and the list of corresponding captions\ntrain_dataset = make_dataset(\n    list(train_descriptions.keys()),\n    list(train_descriptions.values())\n)\n\nvalid_dataset = make_dataset(\n    list(valid_descriptions.keys()),\n    list(valid_descriptions.values())\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:38:13.039185Z","iopub.execute_input":"2022-05-19T17:38:13.039611Z","iopub.status.idle":"2022-05-19T17:38:13.949085Z","shell.execute_reply.started":"2022-05-19T17:38:13.039572Z","shell.execute_reply":"2022-05-19T17:38:13.948312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.  Create arquitecture","metadata":{}},{"cell_type":"code","source":"def get_cnn_model():\n    base_model = efficientnet.EfficientNetB0(\n        input_shape=(*IMAGE_SIZE, 3), include_top=False, weights=\"imagenet\",\n    )\n    # We freeze our feature extractor\n    base_model.trainable = False\n    base_model_out = base_model.output\n    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)\n    cnn_model = keras.models.Model(base_model.input, base_model_out)\n    return cnn_model\n\n\nclass TransformerEncoderBlock(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.attention_1 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim, dropout=0.0\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.dense_1 = layers.Dense(embed_dim, activation=\"relu\")\n\n    def call(self, inputs, training, mask=None):\n        inputs = self.layernorm_1(inputs)\n        inputs = self.dense_1(inputs)\n        attention_output_1 = self.attention_1(\n            query=inputs,\n            value=inputs,\n            key=inputs,\n            attention_mask=None,\n            training=training,\n        )\n        out_1 = self.layernorm_2(inputs + attention_output_1)\n        return out_1\n\nclass PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim=vocab_size, output_dim=embed_dim\n        )\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=embed_dim\n        )\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_tokens = embedded_tokens * self.embed_scale\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n\n\nclass TransformerDecoderBlock(layers.Layer):\n    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n    \n        self.embed_dim = embed_dim\n        self.ff_dim = ff_dim\n        self.num_heads = num_heads\n        self.attention_1 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n        )\n        self.attention_2 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n        )\n        self.ffn_layer_1 = layers.Dense(ff_dim, activation=\"relu\")\n        self.ffn_layer_2 = layers.Dense(embed_dim)\n\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.layernorm_3 = layers.LayerNormalization()\n\n        self.embedding = PositionalEmbedding(\n            embed_dim=embed_dim, sequence_length=MAX_LENGTH_SEQ, vocab_size=vocab_size\n        )\n        self.out = layers.Dense(vocab_size, activation=\"softmax\")\n\n        self.dropout_1 = layers.Dropout(0.3)\n        self.dropout_2 = layers.Dropout(0.5)\n        self.supports_masking = True\n\n    def call(self, inputs, encoder_outputs, training, mask=None):\n        \n        inputs = self.embedding(inputs)\n        causal_mask = self.get_causal_attention_mask(inputs)\n\n        if mask is not None:\n            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n            combined_mask = tf.minimum(combined_mask, causal_mask)\n\n        attention_output_1 = self.attention_1(\n            query=inputs,\n            value=inputs,\n            key=inputs,\n            attention_mask=combined_mask,\n            training=training,\n        )\n\n        out_1 = self.layernorm_1(inputs + attention_output_1)\n\n        attention_output_2 = self.attention_2(\n            query=out_1,\n            value=encoder_outputs,\n            key=encoder_outputs,\n            attention_mask=padding_mask,\n            training=training,\n        )\n\n        out_2 = self.layernorm_2(out_1 + attention_output_2)\n\n\n        ffn_out = self.ffn_layer_1(out_2)\n        ffn_out = self.dropout_1(ffn_out, training=training)\n        ffn_out = self.ffn_layer_2(ffn_out)\n\n        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)\n        ffn_out = self.dropout_2(ffn_out, training=training)\n\n        preds = self.out(ffn_out)\n\n        return preds\n    \n    \n    def get_causal_attention_mask(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size, sequence_length = input_shape[0], input_shape[1]\n        i = tf.range(sequence_length)[:, tf.newaxis]\n        j = tf.range(sequence_length)\n        mask = tf.cast(i >= j, dtype=\"int32\")\n        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n        mult = tf.concat(\n            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n            axis=0,\n        )\n        return tf.tile(mask, mult)\n\nclass ImageCaptioningModel(keras.Model):\n    def __init__(\n        self, cnn_model, encoder, decoder, num_captions_per_image=5, image_aug=None,\n    ):\n        super().__init__()\n        self.cnn_model = cnn_model\n        self.encoder = encoder\n        self.decoder = decoder\n        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n        self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n        self.num_captions_per_image = num_captions_per_image\n        self.image_aug = image_aug\n\n    def calculate_loss(self, y_true, y_pred, mask):\n        loss = self.loss(y_true, y_pred)\n        mask = tf.cast(mask, dtype=loss.dtype)\n        loss *= mask\n        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n\n    def calculate_accuracy(self, y_true, y_pred, mask):\n        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n        accuracy = tf.math.logical_and(mask, accuracy)\n        accuracy = tf.cast(accuracy, dtype=tf.float32)\n        mask = tf.cast(mask, dtype=tf.float32)\n        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n\n    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):\n        encoder_out = self.encoder(img_embed, training=training)\n        batch_seq_inp = batch_seq[:, :-1]\n        batch_seq_true = batch_seq[:, 1:]\n        mask = tf.math.not_equal(batch_seq_true, 0)\n        batch_seq_pred = self.decoder(\n            batch_seq_inp, encoder_out, training=training, mask=mask\n        )\n        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n        return loss, acc\n\n    def train_step(self, batch_data):\n        batch_img, batch_seq = batch_data\n        batch_loss = 0\n        batch_acc = 0\n\n        if self.image_aug:\n            batch_img = self.image_aug(batch_img)\n\n        # 1. Get image embeddings\n        img_embed = self.cnn_model(batch_img)\n\n        # 2. Pass each of the five captions one by one to the decoder\n        # along with the encoder outputs and compute the loss as well as accuracy\n        # for each caption.\n        for i in range(self.num_captions_per_image):\n            with tf.GradientTape() as tape:\n                loss, acc = self._compute_caption_loss_and_acc(\n                    img_embed, batch_seq[:, i, :], training=True\n                )\n\n                # 3. Update loss and accuracy\n                batch_loss += loss\n                batch_acc += acc\n\n            # 4. Get the list of all the trainable weights\n            train_vars = (\n                self.encoder.trainable_variables + self.decoder.trainable_variables\n            )\n\n            # 5. Get the gradients\n            grads = tape.gradient(loss, train_vars)\n\n            # 6. Update the trainable weights\n            self.optimizer.apply_gradients(zip(grads, train_vars))\n\n        # 7. Update the trackers\n        batch_acc /= float(self.num_captions_per_image)\n        self.loss_tracker.update_state(batch_loss)\n        self.acc_tracker.update_state(batch_acc)\n\n        # 8. Return the loss and accuracy values\n        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n\n    def test_step(self, batch_data):\n        batch_img, batch_seq = batch_data\n        batch_loss = 0\n        batch_acc = 0\n\n        # 1. Get image embeddings\n        img_embed = self.cnn_model(batch_img)\n\n        # 2. Pass each of the five captions one by one to the decoder\n        # along with the encoder outputs and compute the loss as well as accuracy\n        # for each caption.\n        for i in range(self.num_captions_per_image):\n            loss, acc = self._compute_caption_loss_and_acc(\n                img_embed, batch_seq[:, i, :], training=False\n            )\n\n            # 3. Update batch loss and batch accuracy\n            batch_loss += loss\n            batch_acc += acc\n\n        batch_acc /= float(self.num_captions_per_image)\n\n        # 4. Update the trackers\n        self.loss_tracker.update_state(batch_loss)\n        self.acc_tracker.update_state(batch_acc)\n\n        # 5. Return the loss and accuracy values\n        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n\n    @property\n    def metrics(self):\n        # We need to list our metrics here so the `reset_states()` can be\n        # called automatically.\n        return [self.loss_tracker, self.acc_tracker]\n\n    \n# Data augmentation for image data\nimage_augmentation = keras.Sequential(\n    [\n        layers.RandomFlip(\"horizontal\"),\n        layers.RandomRotation(0.2),\n        layers.RandomContrast(0.3),\n    ]\n)\ncnn_model = get_cnn_model()\nencoder = TransformerEncoderBlock(embed_dim=EMBEDDING_DIM, dense_dim=FF_DIM, num_heads=1)\ndecoder = TransformerDecoderBlock(embed_dim=EMBEDDING_DIM, ff_dim=FF_DIM, num_heads=2)\ntransformer_caption_model = ImageCaptioningModel(\n    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:38:16.319886Z","iopub.execute_input":"2022-05-19T17:38:16.320149Z","iopub.status.idle":"2022-05-19T17:38:18.328469Z","shell.execute_reply.started":"2022-05-19T17:38:16.320119Z","shell.execute_reply":"2022-05-19T17:38:18.32774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.  Train","metadata":{}},{"cell_type":"code","source":"# Define the loss function\ncross_entropy = keras.losses.SparseCategoricalCrossentropy(\n    from_logits=False, reduction=\"none\"\n)\n\n# EarlyStopping criteria\nearly_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n\n\n# Learning Rate Scheduler for the optimizer\nclass LRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, post_warmup_learning_rate, warmup_steps):\n        super().__init__()\n        self.post_warmup_learning_rate = post_warmup_learning_rate\n        self.warmup_steps = warmup_steps\n\n    def __call__(self, step):\n        global_step = tf.cast(step, tf.float32)\n        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n        warmup_progress = global_step / warmup_steps\n        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress\n        return tf.cond(\n            global_step < warmup_steps,\n            lambda: warmup_learning_rate,\n            lambda: self.post_warmup_learning_rate,\n        )\n\n\n# Create a learning rate schedule\nnum_train_steps = len(train_descriptions) * EPOCHS\nnum_warmup_steps = num_train_steps // 15\nlr_schedule = LRSchedule(post_warmup_learning_rate=1e-4, warmup_steps=num_warmup_steps)\n\n# Compile the model\ntransformer_caption_model.compile(optimizer=tf.keras.optimizers.Adam(lr_schedule), loss=cross_entropy)\n\nmodel_path = os.path.join(\".\",'./caption-model-transformer.hdf5')\n\n# Fit the model\n\ntransformer_caption_model.fit(\n    train_dataset,\n    epochs=EPOCHS,\n    validation_data=valid_dataset,\n    callbacks=[early_stopping],\n)\n\ntransformer_caption_model.save_weights(model_path)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:39:20.882541Z","iopub.execute_input":"2022-05-19T17:39:20.883355Z","iopub.status.idle":"2022-05-19T20:30:44.296247Z","shell.execute_reply.started":"2022-05-19T17:39:20.883313Z","shell.execute_reply":"2022-05-19T20:30:44.295461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.  Evaluate","metadata":{}},{"cell_type":"code","source":"import nltk\n\nB_1_score = 0\nB_2_score = 0\nB_3_score = 0\nB_4_score = 0\n\ncorpus_hypothesis = list()\ncorpus_references = list()\n\nidx = 0\n\nn_samples = len(test_descriptions.keys())\n\nfor image, desc in tqdm(test_descriptions.items()):\n    # Read the image from the disk\n    sample_img = decode_and_resize(image)\n\n    # Pass the image to the CNN\n    img = tf.expand_dims(sample_img, 0)\n    img = transformer_caption_model.cnn_model(img)\n\n    # Pass the image features to the Transformer encoder\n    encoded_img = transformer_caption_model.encoder(img, training=False)\n\n    # Generate the caption using the Transformer decoder\n    decoded_caption = START\n    for i in range(max_decoded_sentence_length):\n        tokenized_caption = vectorization([decoded_caption])[:, :-1]\n        mask = tf.math.not_equal(tokenized_caption, 0)\n        predictions = transformer_caption_model.decoder(\n            tokenized_caption, encoded_img, training=False, mask=mask\n        )\n        sampled_token_index = np.argmax(predictions[0, i, :])\n        sampled_token = index_lookup[sampled_token_index]\n        if sampled_token == STOP:\n            break\n        decoded_caption += \" \" + sampled_token\n\n    decoded_caption = decoded_caption.replace(\"PRINCIPIO \", \"\")\n    decoded_caption = decoded_caption.replace(\" FIN\", \"\").strip()\n    corpus_hypothesis.append(decoded_caption.split())\n    references = [d.split() for d in desc]\n    corpus_references.append(references)\n\n    B1_score += nltk.translate.bleu_score.corpus_bleu(corpus_references, corpus_hypothesis)\n    \n    idx+=1\n    if idx==10:\n        break\n    #B_1_score += nltk.translate.bleu_score.sentence_bleu(references, y_hat, weights=[(1.0)])\n    #B_2_score += nltk.translate.bleu_score.sentence_bleu(references, y_hat, weights=(0.5, 0.5))\n    #B_3_score += nltk.translate.bleu_score.sentence_bleu(references, y_hat, weights=(1.0/3.0, 1.0/3.0, 1.0/3.0))\n    #B_4_score += nltk.translate.bleu_score.sentence_bleu(references, y_hat, weights=(0.25, 0.25, 0.25, 0.25))\n    \nprint(B_1_score/n_samples)\n#print(B_2_score/n_samples)\n#print(B_3_score/n_samples)\n#print(B_4_score/n_samples)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T17:52:30.442471Z","iopub.execute_input":"2022-05-04T17:52:30.443198Z","iopub.status.idle":"2022-05-04T17:52:47.43147Z","shell.execute_reply.started":"2022-05-04T17:52:30.443143Z","shell.execute_reply":"2022-05-04T17:52:47.430757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Generate captions","metadata":{}},{"cell_type":"code","source":"vocab = vectorization.get_vocabulary()\nindex_lookup = dict(zip(range(len(vocab)), vocab))\nmax_decoded_sentence_length = MAX_LENGTH_SEQ - 1\nvalid_images = list(valid_descriptions.keys())\n\ndef generate_caption():\n    # Select a random image from the validation dataset\n    sample_img = np.random.choice(valid_images)\n    print(sample_img)\n\n    # Read the image from the disk\n    sample_img = decode_and_resize(sample_img)\n    img = sample_img.numpy().clip(0, 255).astype(np.uint8)\n    plt.imshow(img)\n    plt.show()\n\n    # Pass the image to the CNN\n    img = tf.expand_dims(sample_img, 0)\n    img = transformer_caption_model.cnn_model(img)\n\n    # Pass the image features to the Transformer encoder\n    encoded_img = transformer_caption_model.encoder(img, training=False)\n\n    # Generate the caption using the Transformer decoder\n    decoded_caption = START\n    for i in range(max_decoded_sentence_length):\n        tokenized_caption = vectorization([decoded_caption])[:, :-1]\n        mask = tf.math.not_equal(tokenized_caption, 0)\n        predictions = transformer_caption_model.decoder(\n            tokenized_caption, encoded_img, training=False, mask=mask\n        )\n        sampled_token_index = np.argmax(predictions[0, i, :])\n        sampled_token = index_lookup[sampled_token_index]\n        decoded_caption += \" \" + sampled_token\n        if sampled_token == STOP:\n            break\n        \n\n    decoded_caption = decoded_caption.replace(START, \"\")\n    decoded_caption = decoded_caption.replace(STOP, \"\")\n    print(\"Predicted Caption:\", decoded_caption.strip())\n\n\n# Check predictions for a few samples\ngenerate_caption()\n#generate_caption()\n#generate_caption()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T20:33:36.658993Z","iopub.execute_input":"2022-05-19T20:33:36.659484Z","iopub.status.idle":"2022-05-19T20:33:37.212489Z","shell.execute_reply.started":"2022-05-19T20:33:36.659448Z","shell.execute_reply":"2022-05-19T20:33:37.2117Z"},"trusted":true},"execution_count":null,"outputs":[]}]}