{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "viki.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW3ovy-eCckU",
        "outputId": "b5fcf67e-61fa-4b00-c8fb-a2420a2dee0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n",
            "170508288/170498071 [==============================] - 11s 0us/step\n",
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 32, 32, 64)   1792        ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 32, 32, 64)  256         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 32, 32, 64)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 32, 32, 64)   36928       ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 32, 32, 64)   256         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 32, 32, 64)  256         ['conv2d_1[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 32, 32, 64)   0           ['conv2d_2[0][0]',               \n",
            "                                                                  'batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 32, 32, 64)   0           ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 32, 32, 64)   36928       ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 32, 32, 64)  256         ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 32, 32, 64)   0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 32, 32, 64)   36928       ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 32, 32, 64)   4160        ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 32, 32, 64)  256         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 32, 32, 64)   0           ['conv2d_5[0][0]',               \n",
            "                                                                  'batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 32, 32, 64)   0           ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 32, 32, 128)  73856       ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 32, 32, 128)  512        ['conv2d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 32, 32, 128)  0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 32, 32, 128)  147584      ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 32, 32, 128)  8320        ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 32, 32, 128)  512        ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 32, 32, 128)  0           ['conv2d_8[0][0]',               \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 32, 32, 128)  0           ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 32, 32, 128)  147584      ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 32, 32, 128)  512        ['conv2d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 32, 32, 128)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 32, 32, 128)  147584      ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 32, 32, 128)  16512       ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 32, 32, 128)  512        ['conv2d_10[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 32, 32, 128)  0           ['conv2d_11[0][0]',              \n",
            "                                                                  'batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 32, 32, 128)  0           ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 16, 16, 128)  0           ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 16, 16, 256)  295168      ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 16, 16, 256)  1024       ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 16, 16, 256)  0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 16, 16, 256)  590080      ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 16, 16, 256)  33024       ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 16, 16, 256)  1024       ['conv2d_13[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 16, 16, 256)  0           ['conv2d_14[0][0]',              \n",
            "                                                                  'batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 16, 16, 256)  0           ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 16, 16, 256)  590080      ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 16, 16, 256)  1024       ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 16, 16, 256)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 16, 16, 256)  590080      ['activation_10[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 16, 16, 256)  65792       ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 16, 16, 256)  1024       ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 16, 16, 256)  0           ['conv2d_17[0][0]',              \n",
            "                                                                  'batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 16, 16, 256)  0           ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 256)   0           ['activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 8, 8, 512)    1180160     ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 8, 8, 512)   2048        ['conv2d_18[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 8, 8, 512)    0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 8, 8, 512)    2359808     ['activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 8, 8, 512)    131584      ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 8, 8, 512)   2048        ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 8, 8, 512)    0           ['conv2d_20[0][0]',              \n",
            "                                                                  'batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 8, 8, 512)    0           ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 8, 8, 512)    2359808     ['activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 8, 8, 512)   2048        ['conv2d_21[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 8, 8, 512)    0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 8, 8, 512)    2359808     ['activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 8, 8, 512)    262656      ['activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 8, 8, 512)   2048        ['conv2d_22[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 8, 8, 512)    0           ['conv2d_23[0][0]',              \n",
            "                                                                  'batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 8, 8, 512)    0           ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " average_pooling2d (AveragePool  (None, 1, 1, 512)   0           ['activation_15[0][0]']          \n",
            " ing2D)                                                                                           \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 512)          0           ['average_pooling2d[0][0]']      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 10)           5130        ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 10)           0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 11,496,970\n",
            "Trainable params: 11,489,290\n",
            "Non-trainable params: 7,680\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "390/390 [==============================] - 215s 512ms/step - loss: 1.5639 - accuracy: 0.4252 - val_loss: 2.3637 - val_accuracy: 0.2407 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "390/390 [==============================] - 196s 501ms/step - loss: 1.0576 - accuracy: 0.6224 - val_loss: 1.7025 - val_accuracy: 0.4907 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "390/390 [==============================] - 196s 501ms/step - loss: 0.8255 - accuracy: 0.7112 - val_loss: 1.1801 - val_accuracy: 0.6480 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "390/390 [==============================] - 195s 498ms/step - loss: 0.6743 - accuracy: 0.7649 - val_loss: 0.9312 - val_accuracy: 0.7217 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "390/390 [==============================] - 195s 499ms/step - loss: 0.5773 - accuracy: 0.8012 - val_loss: 0.8074 - val_accuracy: 0.7351 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "390/390 [==============================] - 195s 499ms/step - loss: 0.5139 - accuracy: 0.8235 - val_loss: 0.7767 - val_accuracy: 0.7650 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "390/390 [==============================] - 195s 499ms/step - loss: 0.4614 - accuracy: 0.8411 - val_loss: 0.6502 - val_accuracy: 0.8010 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "390/390 [==============================] - 195s 499ms/step - loss: 0.4257 - accuracy: 0.8527 - val_loss: 0.6748 - val_accuracy: 0.7919 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "390/390 [==============================] - 195s 499ms/step - loss: 0.3917 - accuracy: 0.8647 - val_loss: 0.5967 - val_accuracy: 0.8133 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "390/390 [==============================] - 195s 499ms/step - loss: 0.3602 - accuracy: 0.8746 - val_loss: 0.6677 - val_accuracy: 0.8124 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "390/390 [==============================] - 195s 498ms/step - loss: 0.3357 - accuracy: 0.8843 - val_loss: 0.6766 - val_accuracy: 0.8031 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "390/390 [==============================] - 194s 497ms/step - loss: 0.3172 - accuracy: 0.8903 - val_loss: 0.5188 - val_accuracy: 0.8338 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "390/390 [==============================] - 195s 498ms/step - loss: 0.2947 - accuracy: 0.8983 - val_loss: 0.5032 - val_accuracy: 0.8471 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "390/390 [==============================] - 195s 499ms/step - loss: 0.2828 - accuracy: 0.9011 - val_loss: 0.5641 - val_accuracy: 0.8339 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "390/390 [==============================] - 195s 499ms/step - loss: 0.2622 - accuracy: 0.9084 - val_loss: 0.3980 - val_accuracy: 0.8699 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "390/390 [==============================] - 195s 498ms/step - loss: 0.2499 - accuracy: 0.9139 - val_loss: 0.4496 - val_accuracy: 0.8648 - lr: 0.0010\n",
            "Epoch 17/50\n",
            "390/390 [==============================] - 195s 498ms/step - loss: 0.2318 - accuracy: 0.9178 - val_loss: 0.4223 - val_accuracy: 0.8720 - lr: 0.0010\n",
            "Epoch 18/50\n",
            "390/390 [==============================] - 201s 514ms/step - loss: 0.2268 - accuracy: 0.9206 - val_loss: 0.4884 - val_accuracy: 0.8527 - lr: 0.0010\n",
            "Epoch 19/50\n",
            "390/390 [==============================] - 194s 496ms/step - loss: 0.2085 - accuracy: 0.9270 - val_loss: 0.6246 - val_accuracy: 0.8410 - lr: 0.0010\n",
            "Epoch 20/50\n",
            "390/390 [==============================] - 194s 495ms/step - loss: 0.2020 - accuracy: 0.9287 - val_loss: 0.5838 - val_accuracy: 0.8445 - lr: 0.0010\n",
            "Epoch 21/50\n",
            "390/390 [==============================] - 194s 496ms/step - loss: 0.1934 - accuracy: 0.9327 - val_loss: 0.4982 - val_accuracy: 0.8554 - lr: 0.0010\n",
            "Epoch 22/50\n",
            "390/390 [==============================] - 194s 495ms/step - loss: 0.1860 - accuracy: 0.9353 - val_loss: 0.3991 - val_accuracy: 0.8832 - lr: 0.0010\n",
            "Epoch 23/50\n",
            "390/390 [==============================] - 194s 495ms/step - loss: 0.1740 - accuracy: 0.9386 - val_loss: 0.3714 - val_accuracy: 0.8885 - lr: 0.0010\n",
            "Epoch 24/50\n",
            "390/390 [==============================] - 193s 494ms/step - loss: 0.1642 - accuracy: 0.9419 - val_loss: 0.3332 - val_accuracy: 0.8993 - lr: 0.0010\n",
            "Epoch 25/50\n",
            "390/390 [==============================] - 193s 494ms/step - loss: 0.1587 - accuracy: 0.9446 - val_loss: 0.3564 - val_accuracy: 0.8931 - lr: 0.0010\n",
            "Epoch 26/50\n",
            "390/390 [==============================] - 193s 495ms/step - loss: 0.1552 - accuracy: 0.9462 - val_loss: 0.4437 - val_accuracy: 0.8838 - lr: 0.0010\n",
            "Epoch 27/50\n",
            "390/390 [==============================] - 193s 495ms/step - loss: 0.1478 - accuracy: 0.9491 - val_loss: 0.3362 - val_accuracy: 0.8986 - lr: 0.0010\n",
            "Epoch 28/50\n",
            "390/390 [==============================] - 193s 495ms/step - loss: 0.1414 - accuracy: 0.9510 - val_loss: 0.3991 - val_accuracy: 0.8909 - lr: 0.0010\n",
            "Epoch 29/50\n",
            "390/390 [==============================] - 193s 494ms/step - loss: 0.1420 - accuracy: 0.9500 - val_loss: 0.4027 - val_accuracy: 0.8936 - lr: 0.0010\n",
            "Epoch 30/50\n",
            "390/390 [==============================] - 193s 494ms/step - loss: 0.1338 - accuracy: 0.9532 - val_loss: 0.4074 - val_accuracy: 0.8939 - lr: 0.0010\n",
            "Epoch 31/50\n",
            "390/390 [==============================] - 194s 495ms/step - loss: 0.1264 - accuracy: 0.9553 - val_loss: 0.6151 - val_accuracy: 0.8598 - lr: 0.0010\n",
            "Epoch 32/50\n",
            "390/390 [==============================] - 194s 496ms/step - loss: 0.1243 - accuracy: 0.9567 - val_loss: 0.4662 - val_accuracy: 0.8807 - lr: 0.0010\n",
            "Epoch 33/50\n",
            "390/390 [==============================] - 194s 496ms/step - loss: 0.1201 - accuracy: 0.9578 - val_loss: 0.4307 - val_accuracy: 0.8820 - lr: 0.0010\n",
            "Epoch 34/50\n",
            "390/390 [==============================] - 194s 496ms/step - loss: 0.1122 - accuracy: 0.9605 - val_loss: 0.6052 - val_accuracy: 0.8662 - lr: 0.0010\n",
            "Epoch 35/50\n",
            "390/390 [==============================] - 194s 496ms/step - loss: 0.0721 - accuracy: 0.9748 - val_loss: 0.2855 - val_accuracy: 0.9237 - lr: 1.0000e-04\n",
            "Epoch 36/50\n",
            "390/390 [==============================] - 194s 497ms/step - loss: 0.0551 - accuracy: 0.9817 - val_loss: 0.2844 - val_accuracy: 0.9275 - lr: 1.0000e-04\n",
            "Epoch 37/50\n",
            "390/390 [==============================] - 194s 496ms/step - loss: 0.0482 - accuracy: 0.9841 - val_loss: 0.2759 - val_accuracy: 0.9300 - lr: 1.0000e-04\n",
            "Epoch 38/50\n",
            "390/390 [==============================] - 194s 497ms/step - loss: 0.0442 - accuracy: 0.9854 - val_loss: 0.2882 - val_accuracy: 0.9287 - lr: 1.0000e-04\n",
            "Epoch 39/50\n",
            "390/390 [==============================] - 194s 496ms/step - loss: 0.0421 - accuracy: 0.9855 - val_loss: 0.3031 - val_accuracy: 0.9264 - lr: 1.0000e-04\n",
            "Epoch 40/50\n",
            "390/390 [==============================] - 194s 496ms/step - loss: 0.0406 - accuracy: 0.9858 - val_loss: 0.3083 - val_accuracy: 0.9277 - lr: 1.0000e-04\n",
            "Epoch 41/50\n",
            "390/390 [==============================] - 194s 496ms/step - loss: 0.0361 - accuracy: 0.9881 - val_loss: 0.3050 - val_accuracy: 0.9271 - lr: 1.0000e-04\n",
            "Epoch 42/50\n",
            "390/390 [==============================] - 194s 497ms/step - loss: 0.0336 - accuracy: 0.9886 - val_loss: 0.3045 - val_accuracy: 0.9285 - lr: 1.0000e-04\n",
            "Epoch 43/50\n",
            "390/390 [==============================] - 194s 497ms/step - loss: 0.0329 - accuracy: 0.9892 - val_loss: 0.3072 - val_accuracy: 0.9277 - lr: 1.0000e-04\n",
            "Epoch 44/50\n",
            "390/390 [==============================] - 194s 497ms/step - loss: 0.0319 - accuracy: 0.9890 - val_loss: 0.3121 - val_accuracy: 0.9283 - lr: 1.0000e-04\n",
            "Epoch 45/50\n",
            "390/390 [==============================] - 194s 497ms/step - loss: 0.0310 - accuracy: 0.9892 - val_loss: 0.3069 - val_accuracy: 0.9293 - lr: 1.0000e-04\n",
            "Epoch 46/50\n",
            "390/390 [==============================] - 194s 496ms/step - loss: 0.0292 - accuracy: 0.9899 - val_loss: 0.2976 - val_accuracy: 0.9304 - lr: 1.0000e-04\n",
            "Epoch 47/50\n",
            "390/390 [==============================] - 194s 497ms/step - loss: 0.0290 - accuracy: 0.9901 - val_loss: 0.3331 - val_accuracy: 0.9278 - lr: 1.0000e-04\n",
            "Epoch 48/50\n",
            "390/390 [==============================] - 195s 499ms/step - loss: 0.0267 - accuracy: 0.9904 - val_loss: 0.3143 - val_accuracy: 0.9296 - lr: 1.0000e-05\n",
            "Epoch 49/50\n",
            "390/390 [==============================] - 196s 501ms/step - loss: 0.0269 - accuracy: 0.9909 - val_loss: 0.3133 - val_accuracy: 0.9298 - lr: 1.0000e-05\n",
            "Epoch 50/50\n",
            "390/390 [==============================] - 196s 500ms/step - loss: 0.0245 - accuracy: 0.9921 - val_loss: 0.3100 - val_accuracy: 0.9306 - lr: 1.0000e-05\n",
            "313/313 [==============================] - 16s 52ms/step - loss: 0.3100 - accuracy: 0.9306\n",
            "Test loss: 0.31001272797584534\n",
            "Test accuracy: 0.9305999875068665\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Add\n",
        "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
        "from keras.layers import BatchNormalization as BN\n",
        "from keras.layers import GaussianNoise as GN\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import LearningRateScheduler as LRS\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 50\n",
        "\n",
        "\n",
        "#### LOAD AND TRANSFORM\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "## DEFINE A DATA AUGMENTATION GENERATOR\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    rotation_range=15,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "\n",
        "## DEF A BLOCK RES + CONV + BN + GN + MAXPOOL\n",
        "def resnet_layer(inputs, filters):\n",
        "    x = inputs\n",
        "    x = Conv2D(filters, (3, 3), padding='same')(x)\n",
        "    x = BN()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(filters, (3, 3), padding='same')(x)\n",
        "    x = BN()(x)\n",
        "    previous = Conv2D(filters, (1,1), padding='same')(inputs)\n",
        "    x = Add()([previous,x])\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  \n",
        "## DEF RESNET TOPOLOGY  \n",
        "inputs = Input(shape=x_train.shape[1:])\n",
        "x = resnet_layer(inputs, 64)\n",
        "x = resnet_layer(x, 64)\n",
        "x = resnet_layer(x, 128)\n",
        "x = resnet_layer(x, 128)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = resnet_layer(x, 256)\n",
        "x = resnet_layer(x, 256)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = resnet_layer(x, 512)\n",
        "x = resnet_layer(x, 512)\n",
        "x = AveragePooling2D(pool_size=(8, 8))(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(num_classes)(x)\n",
        "x = Activation('softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=x)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "## OPTIM AND COMPILE\n",
        "opt = Adam(learning_rate=0.001)\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# DEFINE A LEARNING RATE SCHEDULER\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=0.00001)\n",
        "\n",
        "## TRAINING with DA and LRA\n",
        "history=model.fit(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
        "                            steps_per_epoch=len(x_train) / batch_size, \n",
        "                            epochs=epochs,\n",
        "                            validation_data=(x_test, y_test),\n",
        "                            callbacks=[reduce_lr],\n",
        "                            verbose=1)\n",
        "\n",
        "\n",
        "## TEST\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ]
    }
  ]
}