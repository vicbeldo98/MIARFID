{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practica3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCzZkZcDMTso"
      },
      "source": [
        "###**EJERCICIO 1**\n",
        "\n",
        "Implementar,\t usando\t NLTK\t y\t Python,\t el\t algoritmo\t de\t Lesk\t simplificado\t para\t desambiguar\tel\t sentido\t de\tlas\t palabras\t (WSD).\t La\t función\t recibirá\t una\t palabra\t y\tuna\t frase\tque\tla\tcontenga\ty\tdecidirá\tel\tmejor\tsentido\tpara\tesa\tpalabra.\tLas\t frases\tserán\ten\tinglés\t y\t se\t deberá\teliminar\tde\tla\t frase, de\tla\tglosa\t y de\tlos ejemplos\t de\t cada\tsentido\tlas\t‘stopwords’."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyJqhJ9OMROV",
        "outputId": "18ba3c24-c10c-47c1-d011-3155da8846c4"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords as sw\n",
        "\n",
        "sw_english=sw.words('english')\n",
        "\n",
        "\n",
        "def get_signature(synset):\n",
        "  words = set()\n",
        "  for sentence in synset.examples():\n",
        "      for word in nltk.word_tokenize(sentence):\n",
        "        word = wn.morphy(word)\n",
        "        if word not in sw_english and word is not None:\n",
        "          words.add(word)\n",
        "\n",
        "  for word in nltk.word_tokenize(synset.definition()):\n",
        "      word = wn.morphy(word)\n",
        "      if word not in sw_english and word is not None:\n",
        "          words.add(word)\n",
        "  return words\n",
        "\n",
        "\n",
        "# Algoritmo Lesk simplificado\n",
        "def lesk_algorithm(frase, palabra):\n",
        "  sentidos_palabra = wn.synsets(palabra)\n",
        "  mejor_sentido = sentidos_palabra[0]\n",
        "  max_overlap = 0\n",
        "  contexto =  set(wn.morphy(word) for word in nltk.word_tokenize(frase) if wn.morphy(word) not in sw_english and wn.morphy(word) is not None)\n",
        "  for sentido in sentidos_palabra:\n",
        "      signature = get_signature(sentido)\n",
        "      overlap = len(contexto.intersection(signature))\n",
        "      if overlap > max_overlap:\n",
        "          max_overlap = overlap\n",
        "          mejor_sentido = sentido\n",
        "  return mejor_sentido\n",
        "\n",
        "print(lesk_algorithm(\"Yesterday I went to the bank to withdraw the money and the credit card did not work\",'bank'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Synset('depository_financial_institution.n.01')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDgCp6jyMUw_"
      },
      "source": [
        "###**EJERCICIO 2**\n",
        "\n",
        "Implementar un\talgoritmo\tsimilar\tpara\tla\tdesambiguación\tsemántica utilizando Word Embeddings\ty\tuna\tdistancia\tde\tsimilitud\tsemántica como\tla\tdistancia\tcoseno."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RKQCSB-yiEv",
        "outputId": "003aa279-4425-4ae7-ffda-8e5de7d0b178"
      },
      "source": [
        "import nltk\n",
        "nltk.download('word2vec_sample')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Word embeddings\n",
        "import gensim\n",
        "from nltk.data import find\n",
        "import numpy as np\n",
        "\n",
        "# Cargar el modelo de embeding pre-entrenados del NLTK\n",
        "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package word2vec_sample to /root/nltk_data...\n",
            "[nltk_data]   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oYYYd80MV2Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "245eb1ad-2309-49ea-831e-02b12a095555"
      },
      "source": [
        "from nltk.corpus import stopwords as sw\n",
        "\n",
        "sw_english=sw.words('english')\n",
        "\n",
        "\n",
        "def get_context_embedding(sentence):\n",
        "    context = []\n",
        "    for word in nltk.word_tokenize(sentence):\n",
        "        word = wn.morphy(word)\n",
        "        if word is not None and word not in sw_english and model[word] is not None:\n",
        "            context.append(word)\n",
        "    context = sum([model[w] for w in context if model[w] is not None])\n",
        "    return context\n",
        "\n",
        "def get_signature(synset):\n",
        "  words = set()\n",
        "  for sentence in synset.examples():\n",
        "      for word in nltk.word_tokenize(sentence):\n",
        "        word = wn.morphy(word)\n",
        "        if word not in sw_english and word is not None:\n",
        "          words.add(word)\n",
        "\n",
        "  for word in nltk.word_tokenize(synset.definition()):\n",
        "      word = wn.morphy(word)\n",
        "      if word not in sw_english and word is not None:\n",
        "          words.add(word)\n",
        "  return words\n",
        "\n",
        "# Algoritmo Lesk simplificado\n",
        "def lesk_algorithm_embeddings(frase, palabra):\n",
        "  sentidos_palabra = wn.synsets(palabra)\n",
        "  mejor_sentido = sentidos_palabra[0]\n",
        "  max_overlap = 0\n",
        "  contexto = get_context_embedding(frase)\n",
        "  for sentido in sentidos_palabra:\n",
        "      signature_words = list(get_signature(sentido))\n",
        "      signature_embeddings = []\n",
        "      for w in signature_words:\n",
        "          try:\n",
        "            signature_embeddings.append(model[w])\n",
        "          except Exception:\n",
        "              #print('Palabra ' + w + ' no encontrada')\n",
        "              continue\n",
        "      signature_embeddings = sum(signature_embeddings)\n",
        "      overlap = np.dot(contexto, signature_embeddings) / (np.linalg.norm(contexto) * np.linalg.norm(signature_embeddings))\n",
        "      if overlap > max_overlap:\n",
        "          max_overlap = overlap\n",
        "          mejor_sentido = sentido\n",
        "  \n",
        "  return mejor_sentido\n",
        "\n",
        "frases=[(\"Yesterday I went to the bank to withdraw the money and the credit card did not work\",'bank'), # depository_financial_institution.n.01\n",
        "        (\"The river overflowed the bank.\",'bank'), # bank.n.01 \n",
        "        (\"The van pulled up outside the bank and three masked men got out.\",'bank'), # bank.n.06\n",
        "        (\"The boy leapt from the bank into the cold water.\",'bank'), # bank.n.01\n",
        "        (\"I went fishing for some sea bass.\",'bass'), # bass.n.08\n",
        "        (\"The bass line of the song is too weak.\",'bass'), # bass.s.01\n",
        "        ]\n",
        "\n",
        "for frase in frases:\n",
        "    print(lesk_algorithm_embeddings(frase[0], frase[1]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('depository_financial_institution.n.01')\n",
            "Synset('bank.n.01')\n",
            "Synset('bank.n.06')\n",
            "Synset('bank.n.01')\n",
            "Synset('bass.n.08')\n",
            "Synset('bass.s.01')\n"
          ]
        }
      ]
    }
  ]
}